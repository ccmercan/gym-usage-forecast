name: Scrape & Notify

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes
  workflow_dispatch:  # Manual trigger

jobs:
  scrape-and-alert:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk-bridge2.0-0 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2
      
      - name: Install Python dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps chromium
      
      - name: Run scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          EMAIL_API_KEY: ${{ secrets.EMAIL_API_KEY }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        run: python -m cli ingest
      
      - name: Check and send alerts
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          EMAIL_API_KEY: ${{ secrets.EMAIL_API_KEY }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        run: python -m cli alert

  daily-digest:
    runs-on: ubuntu-latest
    schedule:
      - cron: '0 7 * * *'  # Daily at 7 AM UTC (adjust for your timezone)
    steps:
      - uses: actions/checkout@v3
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install Python dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Send daily digest
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          EMAIL_API_KEY: ${{ secrets.EMAIL_API_KEY }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        run: python -m cli digest

